{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5441,"databundleVersionId":38425,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages & Load Data","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras import layers, models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:21:03.499764Z","iopub.execute_input":"2025-04-22T07:21:03.499967Z","iopub.status.idle":"2025-04-22T07:21:03.504322Z","shell.execute_reply.started":"2025-04-22T07:21:03.499951Z","shell.execute_reply":"2025-04-22T07:21:03.503615Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Unzip the training images\nzip_path = \"/kaggle/input/dogs-vs-cats-redux-kernels-edition/train.zip\"\nextract_path = \"/kaggle/working/train\"\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_path)\n\n# Move images into cats/ and dogs/ folders\nos.makedirs(f\"{extract_path}/cats\", exist_ok=True)\nos.makedirs(f\"{extract_path}/dogs\", exist_ok=True)\n\nimage_dir = os.path.join(extract_path, \"train\")  # actual image folder inside zip\nfor fname in os.listdir(image_dir):\n    src = os.path.join(image_dir, fname)\n    if fname.startswith(\"cat\"):\n        shutil.move(src, os.path.join(extract_path, \"cats\", fname))\n    elif fname.startswith(\"dog\"):\n        shutil.move(src, os.path.join(extract_path, \"dogs\", fname))\n\n# Clean up leftover folder\nshutil.rmtree(image_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:38:34.831593Z","iopub.execute_input":"2025-04-22T05:38:34.831865Z","iopub.status.idle":"2025-04-22T05:38:41.964002Z","shell.execute_reply.started":"2025-04-22T05:38:34.831836Z","shell.execute_reply":"2025-04-22T05:38:41.963076Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Preprocess with ImageDataGenerator","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\ntrain_gen = datagen.flow_from_directory(\n    extract_path,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nval_gen = datagen.flow_from_directory(\n    extract_path,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T19:33:33.953063Z","iopub.execute_input":"2025-04-21T19:33:33.953681Z","iopub.status.idle":"2025-04-21T19:33:34.185421Z","shell.execute_reply.started":"2025-04-21T19:33:33.953658Z","shell.execute_reply":"2025-04-21T19:33:34.184894Z"}},"outputs":[{"name":"stdout","text":"Found 20000 images belonging to 2 classes.\nFound 5000 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# CNN Model","metadata":{}},{"cell_type":"code","source":"model = models.Sequential([\n    layers.Input(shape=(150, 150, 3)),\n    \n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # output is probability of dog\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:43:12.008767Z","iopub.execute_input":"2025-04-21T08:43:12.008962Z","iopub.status.idle":"2025-04-21T08:43:15.907503Z","shell.execute_reply.started":"2025-04-21T08:43:12.008946Z","shell.execute_reply":"2025-04-21T08:43:15.906737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:44:38.000299Z","iopub.execute_input":"2025-04-21T08:44:38.001074Z","iopub.status.idle":"2025-04-21T08:44:38.013516Z","shell.execute_reply.started":"2025-04-21T08:44:38.001049Z","shell.execute_reply":"2025-04-21T08:44:38.012977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Train the Model\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:44:54.417813Z","iopub.execute_input":"2025-04-21T08:44:54.418509Z","iopub.status.idle":"2025-04-21T08:50:36.086229Z","shell.execute_reply.started":"2025-04-21T08:44:54.418483Z","shell.execute_reply":"2025-04-21T08:50:36.085671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Training Curves\nimport matplotlib.pyplot as plt\n\n# Accuracy\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.legend()\nplt.title(\"Model Accuracy Over Epochs\")\nplt.show()\n\n# Loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title(\"Model Loss Over Epochs\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:50:36.087413Z","iopub.execute_input":"2025-04-21T08:50:36.087647Z","iopub.status.idle":"2025-04-21T08:50:36.456821Z","shell.execute_reply.started":"2025-04-21T08:50:36.087631Z","shell.execute_reply":"2025-04-21T08:50:36.455916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tuning","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\nmodel = models.Sequential([\n    layers.Input(shape=(150, 150, 3)),\n    \n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Flatten(),\n    layers.Dropout(0.5),  # ğŸ”¥ NEW: turns off 50% neurons\n    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dense(1, activation='sigmoid')\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:43:21.990139Z","iopub.execute_input":"2025-04-21T09:43:21.990420Z","iopub.status.idle":"2025-04-21T09:43:24.549300Z","shell.execute_reply.started":"2025-04-21T09:43:21.990397Z","shell.execute_reply":"2025-04-21T09:43:24.548750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:43:30.227554Z","iopub.execute_input":"2025-04-21T09:43:30.228285Z","iopub.status.idle":"2025-04-21T09:43:30.236157Z","shell.execute_reply.started":"2025-04-21T09:43:30.228257Z","shell.execute_reply":"2025-04-21T09:43:30.235443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:43:32.004929Z","iopub.execute_input":"2025-04-21T09:43:32.005199Z","iopub.status.idle":"2025-04-21T09:43:32.010773Z","shell.execute_reply.started":"2025-04-21T09:43:32.005179Z","shell.execute_reply":"2025-04-21T09:43:32.010277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=20,  \n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:43:35.081070Z","iopub.execute_input":"2025-04-21T09:43:35.081340Z","iopub.status.idle":"2025-04-21T09:54:08.416374Z","shell.execute_reply.started":"2025-04-21T09:43:35.081319Z","shell.execute_reply":"2025-04-21T09:54:08.415583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Training Curves\nimport matplotlib.pyplot as plt\n\n# Accuracy\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.legend()\nplt.title(\"Model Accuracy Over Epochs\")\nplt.show()\n\n# Loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title(\"Model Loss Over Epochs\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:54:08.417754Z","iopub.execute_input":"2025-04-21T09:54:08.418286Z","iopub.status.idle":"2025-04-21T09:54:08.767999Z","shell.execute_reply.started":"2025-04-21T09:54:08.418266Z","shell.execute_reply":"2025-04-21T09:54:08.767213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grab one batch from val_gen\nval_imgs, val_labels = next(val_gen)  # val_imgs.shape = (32, 150, 150, 3)\n\n# Predict probabilities\nval_preds = model.predict(val_imgs).flatten()\nimport matplotlib.pyplot as plt\n\nclass_names = ['Cat', 'Dog']\n\nplt.figure(figsize=(15, 10))\nfor i in range(10):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(val_imgs[i])\n    plt.axis('off')\n    \n    true_label = class_names[int(val_labels[i])]\n    pred_prob = val_preds[i]\n    pred_label = class_names[int(pred_prob > 0.5)]\n    \n    plt.title(f\"Pred: {pred_label}\\nProb: {pred_prob:.2f}\\nTrue: {true_label}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:54:08.768884Z","iopub.execute_input":"2025-04-21T09:54:08.769603Z","iopub.status.idle":"2025-04-21T09:54:10.260658Z","shell.execute_reply.started":"2025-04-21T09:54:08.769577Z","shell.execute_reply":"2025-04-21T09:54:10.259666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-trained models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_size = (300, 300)\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    horizontal_flip=True,\n    rotation_range=15,\n    zoom_range=0.2\n)\n\ntrain_gen = datagen.flow_from_directory(\n    \"/kaggle/working/train\",\n    target_size=img_size,\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nval_gen = datagen.flow_from_directory(\n    \"/kaggle/working/train\",\n    target_size=img_size,\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T19:43:21.142326Z","iopub.execute_input":"2025-04-21T19:43:21.142970Z","iopub.status.idle":"2025-04-21T19:43:21.381158Z","shell.execute_reply.started":"2025-04-21T19:43:21.142948Z","shell.execute_reply":"2025-04-21T19:43:21.380630Z"}},"outputs":[{"name":"stdout","text":"Found 20000 images belonging to 2 classes.\nFound 5000 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Load base model with 300x300 input\nbase_model = EfficientNetB3(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(300, 300, 3)\n)\nbase_model.trainable = False  # freeze initially\n\n# Add custom classification head\nx = base_model.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.5)(x)\noutput = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = models.Model(inputs=base_model.input, outputs=output)\n\n# Compile\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train head\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n]\n\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=5,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T19:43:24.261132Z","iopub.execute_input":"2025-04-21T19:43:24.261400Z","iopub.status.idle":"2025-04-21T20:23:22.661156Z","shell.execute_reply.started":"2025-04-21T19:43:24.261380Z","shell.execute_reply":"2025-04-21T20:23:22.660572Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 763ms/step - accuracy: 0.4942 - loss: 0.7017 - val_accuracy: 0.5000 - val_loss: 0.6947 - learning_rate: 0.0010\nEpoch 2/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 738ms/step - accuracy: 0.5088 - loss: 0.6990 - val_accuracy: 0.5004 - val_loss: 0.7002 - learning_rate: 0.0010\nEpoch 3/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 745ms/step - accuracy: 0.5117 - loss: 0.6981 - val_accuracy: 0.5000 - val_loss: 0.6926 - learning_rate: 0.0010\nEpoch 4/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 747ms/step - accuracy: 0.5137 - loss: 0.6976 - val_accuracy: 0.5000 - val_loss: 0.6917 - learning_rate: 0.0010\nEpoch 5/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 738ms/step - accuracy: 0.5180 - loss: 0.6954 - val_accuracy: 0.5080 - val_loss: 0.6911 - learning_rate: 0.0010\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Unfreeze top layers\nbase_model.trainable = True\n\n# Freeze earlier layers, fine-tune last ~30\nfor layer in base_model.layers[:-30]:\n    layer.trainable = False\n\n# Re-compile with lower learning rate\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fine-tune entire model\nfine_tune_history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T19:31:09.827593Z","iopub.execute_input":"2025-04-21T19:31:09.827891Z","iopub.status.idle":"2025-04-21T19:31:09.834424Z","shell.execute_reply.started":"2025-04-21T19:31:09.827873Z","shell.execute_reply":"2025-04-21T19:31:09.833867Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# ResNet50","metadata":{}},{"cell_type":"code","source":"# STEP 1: Data Generator with Augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_size = (224, 224)\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    horizontal_flip=True,\n    rotation_range=15,\n    zoom_range=0.2,\n    shear_range=0.2\n)\n\ntrain_gen = datagen.flow_from_directory(\n    \"/kaggle/working/train\",   # Make sure cats/ and dogs/ are subfolders here\n    target_size=img_size,\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nval_gen = datagen.flow_from_directory(\n    \"/kaggle/working/train\",\n    target_size=img_size,\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:21:24.283635Z","iopub.execute_input":"2025-04-22T07:21:24.284126Z","iopub.status.idle":"2025-04-22T07:21:24.518018Z","shell.execute_reply.started":"2025-04-22T07:21:24.284101Z","shell.execute_reply":"2025-04-22T07:21:24.517330Z"}},"outputs":[{"name":"stdout","text":"Found 20000 images belonging to 2 classes.\nFound 5000 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# STEP 2: Build ResNet50V2 model with custom head\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras import layers, models\n\nbase_model = ResNet50V2(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(224, 224, 3)\n)\nbase_model.trainable = False  # Freeze initial layers\n\n# Custom head\nx = base_model.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dense(512, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\noutput = layers.Dense(1, activation='sigmoid')(x)\n\n# Final model\nmodel = models.Model(inputs=base_model.input, outputs=output)\n\n# STEP 3: Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n# STEP 4: Add callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:21:28.180542Z","iopub.execute_input":"2025-04-22T07:21:28.180819Z","iopub.status.idle":"2025-04-22T07:21:30.512468Z","shell.execute_reply.started":"2025-04-22T07:21:28.180797Z","shell.execute_reply":"2025-04-22T07:21:30.511692Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745306488.568490     815 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745306488.569118     815 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# STEP 5: Train the classifier head\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=5,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:21:33.258763Z","iopub.execute_input":"2025-04-22T07:21:33.259365Z","iopub.status.idle":"2025-04-22T07:44:28.919586Z","shell.execute_reply.started":"2025-04-22T07:21:33.259331Z","shell.execute_reply":"2025-04-22T07:44:28.918873Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745306506.350836     881 service.cc:148] XLA service 0x78e88c012eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745306506.350872     881 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745306506.350875     881 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745306507.574838     881 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  2/625\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m44s\u001b[0m 71ms/step - accuracy: 0.5938 - loss: 0.7197   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745306514.214300     881 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 443ms/step - accuracy: 0.9627 - loss: 0.1267 - val_accuracy: 0.9786 - val_loss: 0.0596 - learning_rate: 0.0010\nEpoch 2/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 433ms/step - accuracy: 0.9763 - loss: 0.0634 - val_accuracy: 0.9784 - val_loss: 0.0592 - learning_rate: 0.0010\nEpoch 3/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 425ms/step - accuracy: 0.9807 - loss: 0.0508 - val_accuracy: 0.9808 - val_loss: 0.0539 - learning_rate: 0.0010\nEpoch 4/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 426ms/step - accuracy: 0.9814 - loss: 0.0473 - val_accuracy: 0.9774 - val_loss: 0.0689 - learning_rate: 0.0010\nEpoch 5/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.9843 - loss: 0.0437\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 426ms/step - accuracy: 0.9843 - loss: 0.0437 - val_accuracy: 0.9808 - val_loss: 0.0593 - learning_rate: 0.0010\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# STEP 6: Fine-tune the top layers of ResNet\nbase_model.trainable = True\n\n# Optionally freeze earlier layers\nfor layer in base_model.layers[:-30]:\n    layer.trainable = False\n\n# Re-compile with lower LR for fine-tuning\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Continue training\nfine_tune_history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:21:40.504883Z","iopub.execute_input":"2025-04-22T06:21:40.505137Z","iopub.status.idle":"2025-04-22T06:35:37.321047Z","shell.execute_reply.started":"2025-04-22T06:21:40.505120Z","shell.execute_reply":"2025-04-22T06:35:37.320466Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 445ms/step - accuracy: 0.9597 - loss: 0.1139 - val_accuracy: 0.9668 - val_loss: 0.0830 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 431ms/step - accuracy: 0.9772 - loss: 0.0647 - val_accuracy: 0.9758 - val_loss: 0.0656 - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 417ms/step - accuracy: 0.9794 - loss: 0.0545 - val_accuracy: 0.9774 - val_loss: 0.0557 - learning_rate: 0.0010\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# Path to test folder (after proper unzip)\ntest_dir = \"/kaggle/working/test_flat\"  # <- update this if different\ntest_files = sorted(os.listdir(test_dir))\n\nX_test = []\nids = []\n\nfor fname in tqdm(test_files):\n    img_path = os.path.join(test_dir, fname)\n    img = image.load_img(img_path, target_size=(224, 224))  # âœ… match ResNet input\n    img_array = image.img_to_array(img) / 255.0\n    X_test.append(img_array)\n    ids.append(int(fname.split('.')[0]))\n\nX_test = np.array(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T00:47:41.755749Z","iopub.execute_input":"2025-04-22T00:47:41.756189Z","iopub.status.idle":"2025-04-22T00:48:01.418047Z","shell.execute_reply.started":"2025-04-22T00:47:41.756165Z","shell.execute_reply":"2025-04-22T00:48:01.417467Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:17<00:00, 715.29it/s]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Predict all at once\npreds = model.predict(X_test, batch_size=64).flatten()  # Binary probabilities\nimport pandas as pd\n\nsubmission = pd.DataFrame({\n    'id': ids,\n    'label': preds\n})\n\nsubmission = submission.sort_values('id')  # Ensure order\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:20:24.140708Z","iopub.status.idle":"2025-04-22T07:20:24.140968Z","shell.execute_reply.started":"2025-04-22T07:20:24.140841Z","shell.execute_reply":"2025-04-22T07:20:24.140851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final (ResNet50V2 + EfficientNetB3)","metadata":{}},{"cell_type":"code","source":"# STEP 1: Imports\nfrom tensorflow.keras.applications import ResNet50V2, EfficientNetB3\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\n# STEP 2: Data Generators\nimg_size = (224, 224)\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    horizontal_flip=True,\n    rotation_range=15,\n    zoom_range=0.2,\n    shear_range=0.2\n)\n\ntrain_gen = datagen.flow_from_directory(\n    \"/kaggle/working/train\",\n    target_size=img_size,\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nval_gen = datagen.flow_from_directory(\n    \"/kaggle/working/train\",\n    target_size=img_size,\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n\n# STEP 3: Callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1)\n\n# STEP 4: Build Base Model Function\ndef build_model(base_fn):\n    base_model = base_fn(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n    base_model.trainable = False\n    x = base_model.output\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    model = models.Model(inputs=base_model.input, outputs=output)\n    model.compile(optimizer='adam', loss=BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n    return model\n\n# STEP 5: Train ResNet50V2 (no fine-tuning)\nmodel_resnet = build_model(ResNet50V2)\nmodel_resnet.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=[early_stop, reduce_lr])\n\n# STEP 6: Train EfficientNetB3 (no fine-tuning)\nmodel_effnet = build_model(EfficientNetB3)\nmodel_effnet.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=[early_stop, reduce_lr])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:43:17.521437Z","iopub.execute_input":"2025-04-22T10:43:17.522193Z","iopub.status.idle":"2025-04-22T11:11:02.526692Z","shell.execute_reply.started":"2025-04-22T10:43:17.522166Z","shell.execute_reply":"2025-04-22T11:11:02.525446Z"}},"outputs":[{"name":"stdout","text":"Found 20000 images belonging to 2 classes.\nFound 5000 images belonging to 2 classes.\nEpoch 1/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 434ms/step - accuracy: 0.9393 - loss: 0.3696 - val_accuracy: 0.9818 - val_loss: 0.2381 - learning_rate: 0.0010\nEpoch 2/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 419ms/step - accuracy: 0.9809 - loss: 0.2462 - val_accuracy: 0.9826 - val_loss: 0.2359 - learning_rate: 0.0010\nEpoch 3/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.9813 - loss: 0.2445\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 417ms/step - accuracy: 0.9813 - loss: 0.2445 - val_accuracy: 0.9804 - val_loss: 0.2366 - learning_rate: 0.0010\nEpoch 4/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 416ms/step - accuracy: 0.9884 - loss: 0.2334 - val_accuracy: 0.9838 - val_loss: 0.2339 - learning_rate: 5.0000e-04\nEpoch 5/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 421ms/step - accuracy: 0.9867 - loss: 0.2335 - val_accuracy: 0.9860 - val_loss: 0.2299 - learning_rate: 5.0000e-04\nEpoch 1/5\n\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.5065 - loss: 1.0330","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_815/2430601033.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# STEP 6: Train EfficientNetB3 (no fine-tuning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mmodel_effnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEfficientNetB3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mmodel_effnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL InternalError REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/errors_impl.py(462): __init__\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py(53): quick_execute\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py(1683): call_function\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py(251): call_flat\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py(216): call_preflattened\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py(1322): _call_flat\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py(139): call_function\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(878): _call\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(833): __call__\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py(433): evaluate\n  /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py(117): error_handler\n  /usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py(345): fit\n  /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py(117): error_handler\n  /tmp/ipykernel_815/2430601033.py(64): <cell line: 0>\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3553): run_code\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3473): run_ast_nodes\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3257): run_cell_async\n  /usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3030): _run_cell\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(2975): run_cell\n  /usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py(528): run_cell\n  /usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py(383): do_execute\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(730): execute_request\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(406): dispatch_shell\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(499): process_one\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(510): dispatch_queue\n  /usr/lib/python3.11/asyncio/events.py(84): _run\n  /usr/lib/python3.11/asyncio/base_events.py(1936): _run_once\n  /usr/lib/python3.11/asyncio/base_events.py(608): run_forever\n  /usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py(205): start\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py(712): start\n  /usr/local/lib/python3.11/dist-packages/traitlets/config/application.py(992): launch_instance\n  /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py(37): <module>\n  <frozen runpy>(88): _run_code\n  <frozen runpy>(198): _run_module_as_main\n"],"ename":"RuntimeError","evalue":"pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL InternalError REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/errors_impl.py(462): __init__\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py(53): quick_execute\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py(1683): call_function\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py(251): call_flat\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py(216): call_preflattened\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py(1322): _call_flat\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py(139): call_function\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(878): _call\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(833): __call__\n  /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py(433): evaluate\n  /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py(117): error_handler\n  /usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py(345): fit\n  /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py(117): error_handler\n  /tmp/ipykernel_815/2430601033.py(64): <cell line: 0>\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3553): run_code\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3473): run_ast_nodes\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3257): run_cell_async\n  /usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3030): _run_cell\n  /usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(2975): run_cell\n  /usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py(528): run_cell\n  /usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py(383): do_execute\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(730): execute_request\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(406): dispatch_shell\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(499): process_one\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(510): dispatch_queue\n  /usr/lib/python3.11/asyncio/events.py(84): _run\n  /usr/lib/python3.11/asyncio/base_events.py(1936): _run_once\n  /usr/lib/python3.11/asyncio/base_events.py(608): run_forever\n  /usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py(205): start\n  /usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py(712): start\n  /usr/local/lib/python3.11/dist-packages/traitlets/config/application.py(992): launch_instance\n  /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py(37): <module>\n  <frozen runpy>(88): _run_code\n  <frozen runpy>(198): _run_module_as_main\n","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"shutil.rmtree(\"/kaggle/working/test_flat\", ignore_errors=True)\n\nimport zipfile\n\n# Extract directly to /kaggle/working/test_flat\nwith zipfile.ZipFile(\"/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"/kaggle/working\")\n\n# Rename from /test â†’ /test_flat to make it clear\nimport os\nos.rename(\"/kaggle/working/test\", \"/kaggle/working/test_flat\")\n\n\n# STEP 7: TTA + Ensemble Predictions\ntest_dir = \"/kaggle/working/test_flat\"\ntest_files = sorted(os.listdir(test_dir))\nbatch_size = 500\nids = []\nensemble_preds = []\n\nfor i in tqdm(range(0, len(test_files), batch_size)):\n    batch_files = test_files[i:i+batch_size]\n    batch_imgs = []\n    batch_imgs_flipped = []\n    batch_ids = []\n\n    for fname in batch_files:\n        path = os.path.join(test_dir, fname)\n        img = image.load_img(path, target_size=img_size)\n        arr = image.img_to_array(img) / 255.0\n        flipped = np.fliplr(arr)\n\n        batch_imgs.append(arr)\n        batch_imgs_flipped.append(flipped)\n        batch_ids.append(int(fname.split('.')[0]))\n\n    batch_imgs = np.array(batch_imgs)\n    batch_imgs_flipped = np.array(batch_imgs_flipped)\n\n    resnet_orig = model_resnet.predict(batch_imgs, batch_size=64).flatten()\n    resnet_flip = model_resnet.predict(batch_imgs_flipped, batch_size=64).flatten()\n\n    effnet_orig = model_effnet.predict(batch_imgs, batch_size=64).flatten()\n    effnet_flip = model_effnet.predict(batch_imgs_flipped, batch_size=64).flatten()\n\n    resnet_tta = (resnet_orig + resnet_flip) / 2\n    effnet_tta = (effnet_orig + effnet_flip) / 2\n\n    ensemble_pred = (resnet_tta + effnet_tta) / 2\n    ensemble_pred = np.clip(ensemble_pred ** 1.2, 1e-5, 1 - 1e-5)\n\n    ids.extend(batch_ids)\n    ensemble_preds.extend(ensemble_pred)\n\n# STEP 8: Create Submission\nsubmission = pd.DataFrame({'id': ids, 'label': ensemble_preds})\nsubmission = submission.sort_values('id')\nsubmission.to_csv(\"submission_ensemble2.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:11:02.528690Z","iopub.status.idle":"2025-04-22T11:11:02.529009Z","shell.execute_reply.started":"2025-04-22T11:11:02.528847Z","shell.execute_reply":"2025-04-22T11:11:02.528866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"shutil.rmtree(\"/kaggle/working/test_flat\", ignore_errors=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:58:51.371309Z","iopub.execute_input":"2025-04-22T07:58:51.371965Z","iopub.status.idle":"2025-04-22T07:58:51.761830Z","shell.execute_reply.started":"2025-04-22T07:58:51.371931Z","shell.execute_reply":"2025-04-22T07:58:51.761259Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"shutil.rmtree(\"/kaggle/working/test_flat\", ignore_errors=True)\n\nimport zipfile\n\n# Extract directly to /kaggle/working/test_flat\nwith zipfile.ZipFile(\"/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"/kaggle/working\")\n\n# Rename from /test â†’ /test_flat to make it clear\nimport os\nos.rename(\"/kaggle/working/test\", \"/kaggle/working/test_flat\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:58:53.107218Z","iopub.execute_input":"2025-04-22T07:58:53.108034Z","iopub.status.idle":"2025-04-22T07:58:55.687465Z","shell.execute_reply.started":"2025-04-22T07:58:53.108007Z","shell.execute_reply":"2025-04-22T07:58:55.686670Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\n# Path to the test images (ensure this folder has all .jpgs flat)\ntest_dir = \"/kaggle/working/test_flat\"\ntest_files = sorted(os.listdir(test_dir))\n\n# Setup\nbatch_size = 500  # Number of images to load per mini-batch\nids = []\npreds = []\n\n# Predict in batches\nfor i in tqdm(range(0, len(test_files), batch_size)):\n    batch_files = test_files[i:i+batch_size]\n    batch_imgs = []\n    batch_ids = []\n\n    for fname in batch_files:\n        img_path = os.path.join(test_dir, fname)\n        img = image.load_img(img_path, target_size=(224, 224))  # match model input\n        img_array = image.img_to_array(img) / 255.0\n        batch_imgs.append(img_array)\n        batch_ids.append(int(fname.split('.')[0]))\n\n    batch_array = np.array(batch_imgs)\n    batch_preds = model.predict(batch_array, batch_size=64).flatten()\n\n    ids.extend(batch_ids)\n    preds.extend(batch_preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:58:58.671369Z","iopub.execute_input":"2025-04-22T07:58:58.671650Z","iopub.status.idle":"2025-04-22T08:00:27.654093Z","shell.execute_reply.started":"2025-04-22T07:58:58.671628Z","shell.execute_reply":"2025-04-22T08:00:27.653241Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/25 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step \n","output_type":"stream"},{"name":"stderr","text":"  4%|â–         | 1/25 [00:24<09:38, 24.12s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step\n","output_type":"stream"},{"name":"stderr","text":"  8%|â–Š         | 2/25 [00:26<04:24, 11.49s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step\n","output_type":"stream"},{"name":"stderr","text":" 12%|â–ˆâ–        | 3/25 [00:29<02:43,  7.45s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step\n","output_type":"stream"},{"name":"stderr","text":" 16%|â–ˆâ–Œ        | 4/25 [00:32<01:56,  5.55s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\n","output_type":"stream"},{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 5/25 [00:34<01:30,  4.52s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\n","output_type":"stream"},{"name":"stderr","text":" 24%|â–ˆâ–ˆâ–       | 6/25 [00:37<01:13,  3.89s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\n","output_type":"stream"},{"name":"stderr","text":" 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:40<01:02,  3.48s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step\n","output_type":"stream"},{"name":"stderr","text":" 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:42<00:54,  3.23s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step\n","output_type":"stream"},{"name":"stderr","text":" 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:45<00:48,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:48<00:44,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step\n","output_type":"stream"},{"name":"stderr","text":" 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:50<00:40,  2.87s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n","output_type":"stream"},{"name":"stderr","text":" 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:53<00:36,  2.81s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step\n","output_type":"stream"},{"name":"stderr","text":" 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:56<00:33,  2.77s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n","output_type":"stream"},{"name":"stderr","text":" 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:58<00:30,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [01:01<00:27,  2.73s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step\n","output_type":"stream"},{"name":"stderr","text":" 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [01:04<00:24,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step\n","output_type":"stream"},{"name":"stderr","text":" 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [01:07<00:21,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step\n","output_type":"stream"},{"name":"stderr","text":" 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [01:09<00:19,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step\n","output_type":"stream"},{"name":"stderr","text":" 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [01:12<00:16,  2.73s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [01:15<00:13,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step\n","output_type":"stream"},{"name":"stderr","text":" 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [01:18<00:10,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step\n","output_type":"stream"},{"name":"stderr","text":" 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [01:20<00:08,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step\n","output_type":"stream"},{"name":"stderr","text":" 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [01:23<00:05,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step\n","output_type":"stream"},{"name":"stderr","text":" 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [01:26<00:02,  2.73s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:28<00:00,  3.56s/it]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': ids,\n    'label': preds\n})\nsubmission = submission.sort_values('id')\nsubmission.to_csv(\"submission2.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:22:55.103142Z","iopub.execute_input":"2025-04-22T08:22:55.103628Z","iopub.status.idle":"2025-04-22T08:22:55.134352Z","shell.execute_reply.started":"2025-04-22T08:22:55.103602Z","shell.execute_reply":"2025-04-22T08:22:55.133581Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"test_dir = \"/kaggle/working/test_flat\"\n\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\ntest_files = sorted(os.listdir(test_dir))\n\nX_test = []\nids = []\n\nfor fname in tqdm(test_files):\n    img_path = os.path.join(test_dir, fname)\n    img = image.load_img(img_path, target_size=(150, 150))\n    img_array = image.img_to_array(img) / 255.0\n    X_test.append(img_array)\n    ids.append(int(fname.split('.')[0]))\n\nX_test = np.array(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:07:10.901987Z","iopub.execute_input":"2025-04-21T10:07:10.902650Z","iopub.status.idle":"2025-04-21T10:07:10.905919Z","shell.execute_reply.started":"2025-04-21T10:07:10.902626Z","shell.execute_reply":"2025-04-21T10:07:10.905136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Predict all at once\npreds = model.predict(X_test, batch_size=64).flatten()\n\n# Create submission\nsubmission = pd.DataFrame({'id': ids, 'label': preds})\nsubmission = submission.sort_values('id')\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:07:50.550700Z","iopub.execute_input":"2025-04-21T10:07:50.551217Z","iopub.status.idle":"2025-04-21T10:08:19.352808Z","shell.execute_reply.started":"2025-04-21T10:07:50.551194Z","shell.execute_reply":"2025-04-21T10:08:19.352051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict probabilities\npreds_effnet = model_effnet.predict(X_test, batch_size=64).flatten()\npreds_resnet = model_resnet.predict(X_test, batch_size=64).flatten()\n\n# Average (simple ensemble)\nfinal_preds = (preds_effnet + preds_resnet) / 2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}